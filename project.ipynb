{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, nltk\n",
    "import cleantext as ct\n",
    "import pandas as pd\n",
    "from itertools import chain\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import regexp_tokenize\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(string):\n",
    "    return regexp_tokenize(string, pattern=r\"[^\\w\\d\\-''_<>]\", gaps=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(lst):\n",
    "  stemmer = nltk.SnowballStemmer(language=\"english\", ignore_stopwords=True)\n",
    "  stem_words = []\n",
    "  for token in lst:\n",
    "    x = stemmer.stem(token)\n",
    "    stem_words.append(x)\n",
    "  return stem_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(lst):\n",
    "  for token in lst:\n",
    "    stop_words = stopwords.words('english')\n",
    "    filteredInput = []\n",
    "    \n",
    "    for token in lst:\n",
    "      if token not in stop_words:\n",
    "        filteredInput.append(token)\n",
    "        \n",
    "    return filteredInput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning(csv):\n",
    "  data = csv\n",
    "  df = pd.read_csv(data)\n",
    "  # counter = 0\n",
    "  \n",
    "  for i in range(len(df[\"content\"])):\n",
    "    # # rename fake-like types to just fake for simplicity\n",
    "    # if df.loc[i, 'type'] in [\"unreliable\", \"fake\", \"conspiracy\", \"junksci\"]:\n",
    "    #   df.loc[i, 'type'] = \"fake\"\n",
    "    # elif df.loc[i, 'type'] != 'reliable':\n",
    "    #   df = df.drop(i)\n",
    "    #   continue\n",
    "    value = df.loc[i, \"content\"]\n",
    "    value = ct.clean(value, lower=True, no_line_breaks=True, no_numbers=True, no_emails=True, no_urls=True, replace_with_number=\"<NUM>\")\n",
    "    value = tokenize(value)\n",
    "    value = remove_stopwords(value)\n",
    "    value = stem(value)\n",
    "    df.loc[i, \"content\"] = ' '.join(map(str,value))\n",
    "  # df.reset_index(drop=True)\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run():\n",
    "  df = cleaning(\"news_sample.csv\")\n",
    "  df.to_csv(\"cleaned_news_sample.csv\")\n",
    "run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries Brugt\n",
    "### pandas\n",
    "Converter csv til DataFrame, som kan bruges til at manipulere dataen inde i csv filen nemt\n",
    "### nltk\n",
    "Har indbygget pakker til at tokenize text og fjerne stopwords\n",
    "### cleantext\n",
    "Renser texten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(239,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 1420,\n",
       " 726,\n",
       " [('reuter', 3),\n",
       "  ('journalist', 3),\n",
       "  ('myanmar', 3),\n",
       "  ('clinton', 2),\n",
       "  ('immedi', 2),\n",
       "  ('releas', 2),\n",
       "  ('held', 2),\n",
       "  ('free', 2),\n",
       "  ('num', 2),\n",
       "  ('appear', 2),\n",
       "  ('court', 2),\n",
       "  ('former', 1),\n",
       "  ('u', 1),\n",
       "  ('presid', 1),\n",
       "  ('bill', 1),\n",
       "  ('monday', 1),\n",
       "  ('call', 1),\n",
       "  ('two', 1),\n",
       "  ('press', 1),\n",
       "  ('critic', 1),\n",
       "  ('societi', 1),\n",
       "  ('detent', 1),\n",
       "  ('anywher', 1),\n",
       "  ('unaccept', 1),\n",
       "  ('said', 1),\n",
       "  ('twitter', 1),\n",
       "  ('post', 1),\n",
       "  ('accus', 1),\n",
       "  ('report', 1),\n",
       "  ('wa', 1),\n",
       "  ('lone', 1),\n",
       "  ('kyaw', 1),\n",
       "  ('soe', 1),\n",
       "  ('oo', 1),\n",
       "  ('breach', 1),\n",
       "  ('countri', 1),\n",
       "  ('offici', 1),\n",
       "  ('secret', 1),\n",
       "  ('act', 1),\n",
       "  ('little', 1),\n",
       "  ('us', 1),\n",
       "  ('law', 1),\n",
       "  ('coloni', 1),\n",
       "  ('rule', 1),\n",
       "  ('due', 1),\n",
       "  ('main', 1),\n",
       "  ('citi', 1),\n",
       "  ('yangon', 1),\n",
       "  ('wednesday', 1),\n",
       "  ('second', 1),\n",
       "  ('prosecutor', 1),\n",
       "  ('could', 1),\n",
       "  ('request', 1),\n",
       "  ('charg', 1),\n",
       "  ('file', 1)])"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def analyze_data(csv):\n",
    "  df = pd.read_csv(csv)\n",
    "  \n",
    "  urlsInReliable = 0\n",
    "  urlsInFake = 0\n",
    "  datesInFake = 0\n",
    "  datesInReliable = 0\n",
    "  numInFake = 0\n",
    "  numInReliable = 0\n",
    "  isFake = 0\n",
    "  isReliable = 0\n",
    "  \n",
    "  df = df.filter(items=[\"type\", \"content\"])\n",
    "\n",
    "  for col, row in df.iterrows():\n",
    "    isFake = 0\n",
    "    isReliable = 0\n",
    "    if (row[\"type\"] == \"fake\"):\n",
    "      isFake += 1\n",
    "    else:\n",
    "      isReliable += 1\n",
    "    \n",
    "    for word in row[\"content\"].split():\n",
    "      urlsInFake += int(\"<url>\" == word) * isFake\n",
    "      urlsInReliable += int(\"<url>\" == word) * isReliable\n",
    "      \n",
    "      datesInFake += int(\"<date>\" == word) * isFake\n",
    "      datesInReliable += int(\"<date>\" == word) * isReliable\n",
    "      \n",
    "      numInFake += int(\"<num>\" == word) * isFake\n",
    "      numInReliable += int(\"<num>\" == word) * isReliable\n",
    "    \n",
    "    \n",
    "    dictionary = dict()\n",
    "    pattern = re.compile(r\"\\w+\")\n",
    "    lst = re.findall(pattern, row[\"content\"])\n",
    "    \n",
    "    for word in lst:\n",
    "      if not dictionary.get(word):\n",
    "        dictionary[word] = 1\n",
    "      else:\n",
    "        dictionary[word] += 1\n",
    "    \n",
    "    dictionary = sorted(dictionary.items(), key=lambda x: x[1], reverse=True)\n",
    "    dictionary = dictionary[0:100]\n",
    "\n",
    "  \n",
    "  return urlsInFake, urlsInReliable, datesInFake, datesInReliable, numInFake, numInReliable, dictionary\n",
    "    \n",
    "analyze_data(\"cleaned_news_sample.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**counting the number of URLs in the content**\n",
    "- svar her\n",
    "\n",
    "**counting the number of dates in the content**\n",
    "- svar her\n",
    "\n",
    "**counting the number of numeric values in the content**\n",
    "- svar her\n",
    "\n",
    "**determining the 100 more frequent words that appear in the content**\n",
    "- svar her, mangler kode\n",
    "\n",
    "**plot the frequency of the 10000 most frequent words (any interesting patterns?)**\n",
    "- svar her, mangler kode\n",
    "\n",
    "**run the analysis in point 4 and 5 both before and after removing stopwords and applying stemming: do you see any difference?**\n",
    "- svar her, mangler kode\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Make at least three non-trivial observations/discoveries about the data. These observations could be related to outliers, artefacts, or even better: genuinely interesting patterns in the data that could potentially be used for fake-news detection.**\n",
    "- Indeholder subjektivt ladede ord, så som \"stupid\"\n",
    "- Mangler kilder\n",
    "- Stavefejl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Describe how you ended up representing the FakeNewsCorpus dataset (for instance with a Pandas dataframe). Argue for why you chose this design.**\n",
    "- For at repræsentere data har vi valgt Pandas dataFrame. Det har vi gjort fordi det gør det nemt og overskueligt at arbejde med dataen og rense og manipulere med den.\n",
    "\n",
    "**Did you discover any inherent problems with the data while working with it?**\n",
    "- Ikke rigtigt\n",
    "\n",
    "**Report key properties of the data set - for instance through statistics or visualization.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Her splitter vi data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 0: Vi valgte at gruppere kategorierne ind i \"fake\" og "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ex1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
