{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import cleantext as ct\n",
    "\n",
    "import nltk\n",
    "from nltk import regexp_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(string):\n",
    "    return regexp_tokenize(string, pattern=r\"[^\\w\\d\\-''_<>]\", gaps=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(lst):\n",
    "  for token in lst:\n",
    "    stop_words = stopwords.words('english')\n",
    "    filteredInput = []\n",
    "    \n",
    "    for token in lst:\n",
    "      if token not in stop_words:\n",
    "        filteredInput.append(token)\n",
    "        \n",
    "    return filteredInput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stopwordFiltering(input_str):\n",
    "    stemmer = nltk.SnowballStemmer(language=\"english\")\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    # words = word_tokenize(input_str,preserve_line=True)\n",
    "    words = tokenize(input_str)\n",
    "    filtered_words = [stemmer.stem(w) for w in words if w not in stop_words]\n",
    "    return filtered_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(lst):\n",
    "  stemmer = nltk.SnowballStemmer(language=\"english\", ignore_stopwords=True)\n",
    "  stem_words = []\n",
    "  for token in lst:\n",
    "    x = stemmer.stem(token)\n",
    "    stem_words.append(x)\n",
    "  return stem_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning(csv_path):\n",
    "  df = pd.read_csv(csv_path, dtype=str)\n",
    "  \n",
    "  for i in range(len(df[\"content\"])):\n",
    "    if (i % 1000) == 0: print(i)\n",
    "    value = df.loc[i, \"content\"]\n",
    "    value = ct.clean(value, \n",
    "                     fix_unicode = True,\n",
    "                     to_ascii = True,\n",
    "                     lower = True, \n",
    "                     normalize_whitespace = True,\n",
    "                     no_line_breaks = True,\n",
    "                     strip_lines = True,\n",
    "                     keep_two_line_breaks = False,\n",
    "                     no_urls = True,\n",
    "                     no_emails= True,\n",
    "                     no_phone_numbers= True,\n",
    "                     no_numbers= True,\n",
    "                     no_digits= True,\n",
    "                     no_currency_symbols = False,\n",
    "                     no_punct = True,\n",
    "                     no_emoji = True,\n",
    "                     replace_with_url = \"<URL>\",\n",
    "                     replace_with_email = \"<EMAIL>\",\n",
    "                     replace_with_phone_number = \"<PHONE>\",\n",
    "                     replace_with_number = \"<NUM>\",\n",
    "                     replace_with_digit = \"<NUM>\",\n",
    "                     replace_with_currency_symbol = \"<CUR>\",\n",
    "                     replace_with_punct = \"\",\n",
    "                     lang = \"en\")\n",
    "    value = tokenize(value)\n",
    "    value = remove_stopwords(value)\n",
    "    value = stem(value)\n",
    "    #value = stopwordFiltering(value)\n",
    "    df.loc[i, \"content\"] = ' '.join(map(str,value))\n",
    "  return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**counting the number of URLs in the content**\n",
    "- svar her\n",
    "\n",
    "**counting the number of dates in the content**\n",
    "- svar her\n",
    "\n",
    "**counting the number of numeric values in the content**\n",
    "- svar her\n",
    "\n",
    "**determining the 100 more frequent words that appear in the content**\n",
    "- svar her, mangler kode\n",
    "\n",
    "**plot the frequency of the 10000 most frequent words (any interesting patterns?)**\n",
    "- svar her, mangler kode\n",
    "\n",
    "**run the analysis in point 4 and 5 both before and after removing stopwords and applying stemming: do you see any difference?**\n",
    "- svar her, mangler kode\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries Brugt\n",
    "### pandas\n",
    "Converter csv til DataFrame, som kan bruges til at manipulere dataen inde i csv filen nemt\n",
    "### nltk\n",
    "Har indbygget pakker til at tokenize text og fjerne stopwords\n",
    "### cleantext\n",
    "Renser texten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run():\n",
    "  df = cleaning(\"news_sample.csv\")\n",
    "  df.to_csv(\"cleaned_news_sample.csv\", index=False)\n",
    "  # df = cleaning(\"data\\\\995,000_rows.csv\")\n",
    "  # df.to_csv(\"data\\\\clean_995,000_rows.csv\", index=False)\n",
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_data(csv):\n",
    "  df = pd.read_csv(csv)\n",
    "  \n",
    "  urlsInReliable = 0\n",
    "  urlsInFake = 0\n",
    "  datesInFake = 0\n",
    "  datesInReliable = 0\n",
    "  numInFake = 0\n",
    "  numInReliable = 0\n",
    "  isFake = 0\n",
    "  isReliable = 0\n",
    "  \n",
    "  df = df.filter(items=[\"type\", \"content\"])\n",
    "\n",
    "  for col, row in df.iterrows():\n",
    "    isFake = 0\n",
    "    isReliable = 0\n",
    "    if (row[\"type\"] == \"fake\"):\n",
    "      isFake += 1\n",
    "    else:\n",
    "      isReliable += 1\n",
    "    \n",
    "    for word in row[\"content\"].split():\n",
    "      urlsInFake += int(\"<url>\" == word) * isFake\n",
    "      urlsInReliable += int(\"<url>\" == word) * isReliable\n",
    "      \n",
    "      datesInFake += int(\"<date>\" == word) * isFake\n",
    "      datesInReliable += int(\"<date>\" == word) * isReliable\n",
    "      \n",
    "      numInFake += int(\"<num>\" == word) * isFake\n",
    "      numInReliable += int(\"<num>\" == word) * isReliable\n",
    "  \n",
    "  return urlsInFake, urlsInReliable, datesInFake, datesInReliable, numInFake, numInReliable\n",
    "    \n",
    "analyze_data(\"finished.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Make at least three non-trivial observations/discoveries about the data. These observations could be related to outliers, artefacts, or even better: genuinely interesting patterns in the data that could potentially be used for fake-news detection.**\n",
    "- Indeholder subjektivt ladede ord, så som \"stupid\"\n",
    "- Mangler kilder\n",
    "- Stavefejl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Describe how you ended up representing the FakeNewsCorpus dataset (for instance with a Pandas dataframe). Argue for why you chose this design.**\n",
    "- For at repræsentere data har vi valgt Pandas dataFrame. Det har vi gjort fordi det gør det nemt og overskueligt at arbejde med dataen og rense og manipulere med den.\n",
    "\n",
    "**Did you discover any inherent problems with the data while working with it?**\n",
    "- Ikke rigtigt\n",
    "- En artikel i 955.000_row.csv har dato hvor der skal være type. \n",
    "\n",
    "**Report key properties of the data set - for instance through statistics or visualization.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('..\\\\data\\\\995,000_rows.csv')\n",
    "df.type.value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ex1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
