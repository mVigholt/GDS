{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34865/2493697506.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas, re, cleantext, nltk\n",
      "Since the GPL-licensed package `unidecode` is not installed, using Python's `unicodedata` package which yields worse results.\n"
     ]
    }
   ],
   "source": [
    "import pandas, re, cleantext, nltk\n",
    "import pandas as pd\n",
    "from itertools import chain\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(csv):\n",
    "  df = pandas.read_csv(csv)\n",
    "  for i in range(len(df[\"content\"])):\n",
    "    df.loc[i, \"content\"] = cleantext.clean(df.loc[i, \"content\"],\n",
    "      lower=True,\n",
    "      no_line_breaks=True,\n",
    "      no_numbers=True,\n",
    "      no_emails=True,\n",
    "      no_urls=True,\n",
    "      replace_with_number=\"<NUM>\")\n",
    "  return df\n",
    "\n",
    "clean(\"news_sample.csv\").to_csv(\"cleaned_news_sample.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import regexp_tokenize\n",
    "\n",
    "def tokenize(stringInput):\n",
    "              \n",
    "    return regexp_tokenize(stringInput, pattern=r\"\\s|[.,;']\", gaps=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(input):\n",
    "  stemmer = nltk.SnowballStemmer(language=\"english\", ignore_stopwords=True)\n",
    "  stem_words = []\n",
    "  for token in input:\n",
    "    x = stemmer.stem(token)\n",
    "    stem_words.append(x)\n",
    "  return stem_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(cell):\n",
    "  for token in cell:\n",
    "    stop_words = stopwords.words('english')\n",
    "    filteredInput = []\n",
    "    \n",
    "    for token in cell:\n",
    "      if token not in stop_words:\n",
    "        filteredInput.append(token)\n",
    "        \n",
    "    return filteredInput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Must have equal len keys and value when setting with an iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m stopwordsRemoved:\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(x)\n\u001b[0;32m---> 10\u001b[0m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 4\u001b[0m, in \u001b[0;36mrun\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnews_sample.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m cleaned \u001b[38;5;241m=\u001b[39m clean(data)\n\u001b[0;32m----> 4\u001b[0m tokenizedData \u001b[38;5;241m=\u001b[39m \u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcleaned\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m stopwordsRemoved \u001b[38;5;241m=\u001b[39m remove_stopwords(tokenizedData)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m stopwordsRemoved:\n",
      "Cell \u001b[0;32mIn[3], line 17\u001b[0m, in \u001b[0;36mtokenize\u001b[0;34m(dataFrame)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# for cell in df['content']:\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m#     # words = cell.split()\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m#     # allWords.append(list(words))\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m#     tokens = nltk.word_tokenize(cell)\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m#     allWords.append(tokens)\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m])):\n\u001b[0;32m---> 17\u001b[0m     \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m tokensREGEX \u001b[38;5;241m=\u001b[39m regexp_tokenize(df\u001b[38;5;241m.\u001b[39mloc[i, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m], pattern\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms|[.,;\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m, gaps\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Create flat list from list of lists\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# flat_list_raw = [word for inner_list in allWords for word in inner_list]\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# flat_list_final = [word for item in flat_list_raw for word in re.split(r'[().,\\'\"-]', item) if word.strip()]\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# flat_list_final = [word for item in flat_list_raw for word in item.split('.')]\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "File \u001b[0;32m~/miniconda3/envs/GDS_exam/lib/python3.11/site-packages/pandas/core/indexing.py:912\u001b[0m, in \u001b[0;36m_LocationIndexer.__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m    909\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_valid_setitem_indexer(key)\n\u001b[1;32m    911\u001b[0m iloc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miloc\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39miloc\n\u001b[0;32m--> 912\u001b[0m \u001b[43miloc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setitem_with_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/GDS_exam/lib/python3.11/site-packages/pandas/core/indexing.py:1946\u001b[0m, in \u001b[0;36m_iLocIndexer._setitem_with_indexer\u001b[0;34m(self, indexer, value, name)\u001b[0m\n\u001b[1;32m   1943\u001b[0m \u001b[38;5;66;03m# align and set the values\u001b[39;00m\n\u001b[1;32m   1944\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m take_split_path:\n\u001b[1;32m   1945\u001b[0m     \u001b[38;5;66;03m# We have to operate column-wise\u001b[39;00m\n\u001b[0;32m-> 1946\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setitem_with_indexer_split_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1947\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1948\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setitem_single_block(indexer, value, name)\n",
      "File \u001b[0;32m~/miniconda3/envs/GDS_exam/lib/python3.11/site-packages/pandas/core/indexing.py:2002\u001b[0m, in \u001b[0;36m_iLocIndexer._setitem_with_indexer_split_path\u001b[0;34m(self, indexer, value, name)\u001b[0m\n\u001b[1;32m   1997\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(value) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_integer(info_axis):\n\u001b[1;32m   1998\u001b[0m         \u001b[38;5;66;03m# This is a case like df.iloc[:3, [1]] = [0]\u001b[39;00m\n\u001b[1;32m   1999\u001b[0m         \u001b[38;5;66;03m#  where we treat as df.iloc[:3, 1] = 0\u001b[39;00m\n\u001b[1;32m   2000\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setitem_with_indexer((pi, info_axis[\u001b[38;5;241m0\u001b[39m]), value[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m-> 2002\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2003\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMust have equal len keys and value \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2004\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhen setting with an iterable\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2005\u001b[0m     )\n\u001b[1;32m   2007\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m lplane_indexer \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(value) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex):\n\u001b[1;32m   2008\u001b[0m     \u001b[38;5;66;03m# We get here in one case via .loc with a all-False mask\u001b[39;00m\n\u001b[1;32m   2009\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Must have equal len keys and value when setting with an iterable"
     ]
    }
   ],
   "source": [
    "def run():\n",
    "  data = \"news_sample.csv\"\n",
    "  cleaned = clean(data)\n",
    "  tokenizedData = tokenize(cleaned)\n",
    "  stopwordsRemoved = remove_stopwords(tokenizedData)\n",
    "  StemmedData = stem(stopwordsRemoved)\n",
    "  \n",
    "  for x in StemmedData:\n",
    "    print(x)\n",
    "\n",
    "run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ex1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
